{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import import_ipynb\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import transformers\n",
    "import concurrent.futures\n",
    "\n",
    "batch_size = 32\n",
    "max_length = 512 \n",
    "\n",
    "# Labels in our dataset.\n",
    "labels = [\"contradiction\", \"entailment\", \"neutral\"]\n",
    "\n",
    "\n",
    "class BertSemanticDataGenerator(tf.keras.utils.Sequence):\n",
    "    \"\"\"Generates batches of data.\n",
    "\n",
    "    Args:\n",
    "        sentence_pairs: Array as input sentences.\n",
    "        labels: Array of labels.\n",
    "        batch_size: Integer batch size.\n",
    "        shuffle: boolean, whether to shuffle the data.\n",
    "        include_targets: boolean, whether to incude the labels.\n",
    "\n",
    "    Returns:\n",
    "        Tuples `([input_ids, attention_mask, `token_type_ids], labels)`\n",
    "        (or just `[input_ids, attention_mask, `token_type_ids]`\n",
    "         if `include_targets=False`)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        sentence_pairs,\n",
    "        labels,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        include_targets=True,\n",
    "    ):\n",
    "        self.sentence_pairs = sentence_pairs\n",
    "        self.labels = labels\n",
    "        self.shuffle = shuffle\n",
    "        self.batch_size = batch_size\n",
    "        self.include_targets = include_targets\n",
    "        # Load our BERT Tokenizer to encode the text.\n",
    "        # We will use base-base-uncased pretrained model.\n",
    "        self.tokenizer = transformers.BertTokenizer.from_pretrained(\n",
    "            \"bert-base-uncased\", do_lower_case=True\n",
    "        )\n",
    "        self.indexes = np.arange(len(self.sentence_pairs))\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def __len__(self):\n",
    "        # Denotes the number of batches per epoch.\n",
    "        return len(self.sentence_pairs) // self.batch_size\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Retrieves the batch of index.\n",
    "        indexes = self.indexes[idx * self.batch_size : (idx + 1) * self.batch_size]\n",
    "        sentence_pairs = self.sentence_pairs[indexes]\n",
    "\n",
    "        # With BERT tokenizer's batch_encode_plus batch of both the sentences are\n",
    "        # encoded together and separated by [SEP] token.\n",
    "        encoded = self.tokenizer.batch_encode_plus(\n",
    "            sentence_pairs.tolist(),\n",
    "            add_special_tokens=True,\n",
    "            max_length=max_length,\n",
    "            return_attention_mask=True,\n",
    "            return_token_type_ids=True,\n",
    "            pad_to_max_length=True,\n",
    "            return_tensors=\"tf\",\n",
    "        )\n",
    "\n",
    "        # Convert batch of encoded features to numpy array.\n",
    "        input_ids = np.array(encoded[\"input_ids\"], dtype=\"int32\")\n",
    "        attention_masks = np.array(encoded[\"attention_mask\"], dtype=\"int32\")\n",
    "        token_type_ids = np.array(encoded[\"token_type_ids\"], dtype=\"int32\")\n",
    "\n",
    "        # Set to true if data generator is used for training/validation.\n",
    "        if self.include_targets:\n",
    "            labels = np.array(self.labels[indexes], dtype=\"int32\")\n",
    "            return [input_ids, attention_masks, token_type_ids], labels\n",
    "        else:\n",
    "            return [input_ids, attention_masks, token_type_ids]\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        # Shuffle indexes after each epoch if shuffle is set to True.\n",
    "        if self.shuffle:\n",
    "            np.random.RandomState(42).shuffle(self.indexes)\n",
    "            \n",
    "#print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
    "\n",
    "#strategy = tf.distribute.MirroredStrategy()\n",
    "\n",
    "def create_model():\n",
    "    #with strategy.scope():\n",
    "    # Encoded token ids from BERT tokenizer.\n",
    "    input_ids = tf.keras.layers.Input(\n",
    "        shape=(max_length,), dtype=tf.int32, name=\"input_ids\"\n",
    "    )\n",
    "    # Attention masks indicates to the model which tokens should be attended to.\n",
    "    attention_masks = tf.keras.layers.Input(\n",
    "        shape=(max_length,), dtype=tf.int32, name=\"attention_masks\"\n",
    "    )\n",
    "    # Token type ids are binary masks identifying different sequences in the model.\n",
    "    token_type_ids = tf.keras.layers.Input(\n",
    "        shape=(max_length,), dtype=tf.int32, name=\"token_type_ids\"\n",
    "    )\n",
    "    # Loading pretrained BERT model.\n",
    "    bert_model = transformers.TFBertModel.from_pretrained(\"bert-base-uncased\")\n",
    "    \n",
    "    # Freeze the BERT model to reuse the pretrained features without modifying them.\n",
    "    bert_model.trainable = False\n",
    "    \n",
    "    sequence_output, pooled_output = bert_model(\n",
    "        input_ids, attention_mask=attention_masks, token_type_ids=token_type_ids\n",
    "    )\n",
    "    #print(\"sequence_output\")\n",
    "    #print(sequence_output)\n",
    "    #print(\"sequence_output\")\n",
    "    #print(pooled_output)\n",
    "\n",
    "    # Add trainable layers on top of frozen layers to adapt the pretrained features on the new data.\n",
    "    bi_lstm = tf.keras.layers.Bidirectional(\n",
    "        tf.keras.layers.LSTM(256, return_sequences=True)\n",
    "    )(sequence_output)\n",
    "    #print(\"Bi-Lstm.......\")\n",
    "    #print(bi_lstm)\n",
    "    # Applying hybrid pooling approach to bi_lstm sequence output.\n",
    "    avg_pool = tf.keras.layers.GlobalAveragePooling1D()(bi_lstm)\n",
    "    #print(\"avg_pool.......\")\n",
    "    #print(avg_pool)\n",
    "    max_pool = tf.keras.layers.GlobalMaxPooling1D()(bi_lstm)\n",
    "    #print(\"max_pool.......\")\n",
    "    #print(max_pool)\n",
    "    concat = tf.keras.layers.concatenate([avg_pool, max_pool])\n",
    "    #print(\"concat.......\")\n",
    "    #print(concat)\n",
    "    dropout = tf.keras.layers.Dropout(0.3)(concat)\n",
    "    #print(\"dropout.......\")\n",
    "    #print(dropout)\n",
    "    output = tf.keras.layers.Dense(3, activation=\"softmax\")(dropout)\n",
    "    #print(\"output.......\")\n",
    "    #print(output)\n",
    "    model = tf.keras.models.Model(\n",
    "        inputs=[input_ids, attention_masks, token_type_ids], outputs=output\n",
    "    )\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(),\n",
    "        loss=\"categorical_crossentropy\",\n",
    "        metrics=[\"acc\"],\n",
    "    )\n",
    "    return model,bert_model\n",
    "\n",
    "#print(f\"Strategy: {strategy}\")\n",
    "model,bert_model = create_model()\n",
    "#model.summary()\n",
    "\n",
    "l = tf.train.latest_checkpoint(\"model/\")\n",
    "model.load_weights(l)\n",
    "\n",
    "def check_(sentence_pairs):\n",
    "    test_data = BertSemanticDataGenerator(\n",
    "        sentence_pairs, labels=None, batch_size=1, shuffle=False, include_targets=False,\n",
    "    )\n",
    "\n",
    "    proba = model.predict(test_data)[0]\n",
    "    idx = np.argmax(proba)\n",
    "    proba = f\"{proba[idx]: .2f}%\"\n",
    "    pred = labels[idx]\n",
    "    return pred, proba\n",
    "\n",
    "def CheckSim(Teacher,Student):\n",
    "    \n",
    "    ## if Student Answer goes greater than 512 so apply this \n",
    "    s= []\n",
    "    \n",
    "    while len(Student)>512:\n",
    "        stop=1\n",
    "        for i in range(500,512):\n",
    "            if stop and (Student[i]=='.' or Student[i]==',' or Student[i]==' ' or Student[i]==':'): \n",
    "                s.append([Student[:i+1]])\n",
    "                Student = Student[i+1:]\n",
    "                #print(len(Student))\n",
    "                stop=0\n",
    "    s.append(Student)\n",
    "    \n",
    "    sentence_pairs = []\n",
    "    \n",
    "    for i in s:\n",
    "        sentence_pairs.append(np.array([[str(Teacher), str(i)]]))\n",
    "\n",
    "    result = []\n",
    "    grade =  []\n",
    "\n",
    "    with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "        results= executor.map(check_, sentence_pairs)\n",
    "        for i in results:\n",
    "            grade.append(i[0])\n",
    "            result.append(i[1])\n",
    "    \n",
    "    marks = []\n",
    "    for i in range(0,len(grade)):\n",
    "        #print(grade[i])\n",
    "        #print(result[i])\n",
    "        result[i] = int(result[i][3:5])\n",
    "        #print(\"############\")\n",
    "        #print(grade[i])\n",
    "        #print(result[i])\n",
    "        if \"contradiction\" == grade[i]:\n",
    "            if result[i]>=60:\n",
    "                ans=result[i]-60\n",
    "                ans= 40 - ans\n",
    "                marks.append(ans)\n",
    "        if \"neutral\" == grade[i]:\n",
    "            if result[i]>=60:\n",
    "                ans=result[i]-60\n",
    "                ans=(ans/2)+60\n",
    "                marks.append(ans)\n",
    "        if \"entailment\" == grade[i]:\n",
    "            if result[i]>=60:\n",
    "                ans=result[i]-60\n",
    "                ans=(ans/2)+80\n",
    "                marks.append(ans)\n",
    "\n",
    "    n =len(marks)\n",
    "    m=0\n",
    "    #print(\"Printing Marks by threads : {}\".format(marks))\n",
    "    for i in range(0,n):\n",
    "        m = m+marks[i]\n",
    "    return m/n\n",
    "    \n",
    "#sentence1 = \"It should be re-engineered or replaced if suitable system is available, this is so because of its high business value it contributes a lot to the business.\"\n",
    "#sentence2 = \"High Quality with High Business value as the system is somehow important to the business and as the change cannot be avoided further in this system and is running for long time.\"\n",
    "\n",
    "#CheckSim(sentence1, sentence2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
